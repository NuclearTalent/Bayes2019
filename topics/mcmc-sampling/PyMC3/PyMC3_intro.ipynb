{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyMC3 Introduction\n",
    "\n",
    "Last revised 18-Jun-2019 by Dick Furnstahl (furnstahl.1@osu.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A good starting point for notebooks with PyMC3 examples is the official documentation site: https://docs.pymc.io/.  We've adapted some examples from that site here and in other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside.** Here is a good quote from Rob Hicks on HMC and No U-Turn:\n",
    "\n",
    "\"The idea: rather than blindly stumbling around the posterior, use the posterior gradient to skate around the gradient contour. As you skate closer to a drop-off (gradient is steep and probability is lower), potential energy decreases and kinetic energy increases (since energy is always conserved). When this happens the skater is turned back uphill and pushed from the precipice and skates on along a posterior likelihood contour. The No U-Turn sampler keeps skating until the skater tries to turn back towards the original point.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano as tt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "\n",
    "# Recommended: document what PyMC3 version we are using\n",
    "print(f'Running on PyMC3 v{pm.__version__}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic setup of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First we need to create a *model*, which will be an instance of the Model class.  The model has references to all random variables (RVs) and computes the model log posterior (logp) and its gradients.  We typically instantiate it using a with context.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as my_model:\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sigma=1, observed=np.random.randn(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So `my_model` is an instance of the PyMC3 Model class, and we have set up a prior for `mu` in the form of a standard normal distribution (i.e., mean = 0 and standard deviation = 1).  The last line sets up the likelihood, also distributed as a normal with observed data taken as 100 random draw from a standard normal distribution.  The standard deviation sd for the mu posterior is given.  The goal will be to sample the posterior for mu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling\n",
    "\n",
    "The main entry point to MCMC sampling algorithms is via the `pm.sample()` function. By default, this function tries to auto-assign the right sampler(s) and auto-initialize if you don’t pass anything.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you can see, on a continuous model, `PyMC3` assigns the NUTS sampler, which is very efficient even for complex models. `PyMC3` also runs variational inference (i.e. `ADVI`) to find good starting parameters for the sampler. Here we draw 1000 samples from the posterior and allow the sampler to adjust its parameters in an additional 500 iterations. These 500 samples are discarded by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as my_NUTS_model:\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sigma=1, observed=np.random.randn(100))\n",
    "\n",
    "    trace_NUTS = pm.sample(1000, tune=500)\n",
    "\n",
    "pm.traceplot(trace_NUTS);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Available samplers\n",
    "\n",
    "`PyMC3` offers a variety of samplers, found in pm.step_methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "list(filter(lambda x: x[0].isupper(), dir(pm.step_methods)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Commonly used step-methods besides NUTS are Metropolis and Slice. The claim is that for almost all continuous models, ``NUTS`` should be preferred. There are hard-to-sample models for which NUTS will be very slow causing many users to use Metropolis instead. This practice, however, is rarely successful. NUTS is fast on simple models but can be slow if the model is very complex or it is badly initialized. In the case of a complex model that is hard for NUTS, Metropolis, while faster, will have a very low effective sample size or not converge properly at all. A better approach is to instead try to improve initialization of NUTS, or reparameterize the model.\n",
    "\n",
    "For completeness, other sampling methods can be passed to sample.  Here is an example (Metropolis-Hastings):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as my_Metropolis_model:\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sigma=1, observed=np.random.randn(100))\n",
    "\n",
    "    step = pm.Metropolis()\n",
    "    trace_MH = pm.sample(1000, step=step)\n",
    "\n",
    "pm.traceplot(trace_MH);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Analyze sampling results\n",
    "\n",
    "The most common used plot to analyze sampling results is the so-called trace-plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace_NUTS);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sd=1)\n",
    "    sd = pm.HalfNormal('sd', sd=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sd=sd, observed=np.random.randn(100))\n",
    "\n",
    "    step1 = pm.Metropolis(vars=[mu])\n",
    "    step2 = pm.Slice(vars=[sd])\n",
    "    trace_2_samplers = pm.sample(10000, step=[step1, step2], cores=4)\n",
    "\n",
    "pm.traceplot(trace_2_samplers);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.gelman_rubin(trace_MH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace_MH);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples from Rob Hicks\n",
    "\n",
    "See https://rlhick.people.wm.edu/stories/bayesian_7.html.  We also have a notebook from his Bayesian 8 \"story\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We start with a very simple one parameter model and then move to slightly more complicated settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigma = 3.  # standard deviation\n",
    "mu = 10.    # mean\n",
    "num_samples = 100  # 10**6\n",
    "\n",
    "# sample from a normal distribution\n",
    "data = stats.norm(mu, sigma).rvs(num_samples)  \n",
    "\n",
    "# plot a histogram of the sampled data\n",
    "num_bins = 20\n",
    "plt.hist(data, bins=num_bins)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Run the previous cell a few times to see the fluctuations.  Crank up the number of samples to `10**6` to see a smoother histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PyMC3 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We instantiate a Model with a descriptions of priors and the likelihood. Here, `mu` is defined to be a random variable (we want to sample this variable by generating a Markov chain) and we provide a prior distribution with associated hyper-parameters for it. The likelihood function is chosen to be Normal, with one parameter to be estimated (mu), and we use known $\\sigma$ (denoted as `sigma`). Our \"dependent variable\" is given by `observed=data`, where `data` is generated above and shown in the histogram. So we our implementing Bayes theorem in the form:  \n",
    "\\begin{align}\n",
    "   \\newcommand{\\Pr}{\\textrm{pr}} \n",
    "   \\newcommand{\\data}{\\textbf{data}}\n",
    "   \\Pr(\\mu | \\sigma, \\data) \\propto \\Pr(\\data | \\mu, \\sigma) \\times \\Pr(\\mu |\\mu^0_\\mu, \\sigma^0_\\mu)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# parameters for the prior on mu\n",
    "mu_prior = 8.\n",
    "sigma_prior = 1.5  # Note this is our prior on the std of mu\n",
    "\n",
    "# Could do this instead as:\n",
    "#   basic_model = pm3.Model()\n",
    "#   with basic_model:\n",
    "\n",
    "with pm.Model() as basic_model:  \n",
    "\n",
    "    # Prior for unknown model parameters (mean and sd of the normal pdf)\n",
    "    mu = pm.Normal('Mean of Data', mu_prior, sigma_prior)\n",
    "    \n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    data_in = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next we define how the Markov chain will be constructed. The example we are following set `startvals` to be the MAP and used a Metropolis step method.  There always seems to be a complaint with the latest pyMC3 about using find_MAP to start the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "chain_length = 10000\n",
    "\n",
    "with basic_model:\n",
    "    # obtain starting values via MAP (maximum a posteriori)\n",
    "    startvals = pm.find_MAP(model=basic_model)  # model here is optional\n",
    "    print(startvals)\n",
    "    \n",
    "    # instantiate sampler\n",
    "    step = pm.Metropolis()   # Metropolis-Hastings\n",
    "\n",
    "    # draw 10000 posterior samples for each chain (4 chains by default?)\n",
    "    trace = pm.sample(draws=chain_length, step=step, start=startvals) \n",
    "    #trace = pm.sample(chain_length, step=step) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot the four chains\n",
    "pm.traceplot(trace, figsize=(20,5));\n",
    "\n",
    "# Summary information on the Markov chains\n",
    "pm.summary(trace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remember that what we are generating is a posterior for the mean given the data and our (assumed) knowledge of the standard deviation.\n",
    "\n",
    "So for the summary info we get the mean and standard deviation (sd) of the distribution, with an estimate of the Monte Carlo error. What does hpd stand for?  \"Highest posterior density\"  2.5 and 97.5 are percentages, so one talks of a 95% hpd interval in this case. \n",
    "\n",
    "From an answer online: \"You create the parameter trace plots to make sure that your a priori distribution is well calibrated which is indicated by your parameters having sufficient state changes as the MCMC algorithm runs.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"All the results are contained in the trace variable. This is a pymc3 results object. It contains some information that we might want to extract at times. `Varnames` tells us all the variable names setup in our model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trace.varnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This was set up when we initiated our model (in specifying the prior for mu).  With the variable names, we can extract chain values for each variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trace['Mean of Data']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Is this one chain or all four chains?  Check the length!  Looks like all four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(len(trace['Mean of Data']))\n",
    "print(trace['Mean of Data'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now for diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Autocorrelation plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.plots.autocorrplot(trace,figsize=(17,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What do we see here?  An autocorrelation time around 10 or so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Acceptance rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accept = np.sum(trace['Mean of Data'][1:] != trace['Mean of Data'][:-1])\n",
    "print(\"Acceptance Rate: \", accept/trace['Mean of Data'].shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That looks like we have to work harder than one might have expected.  It is taking the array of results and comparing each point to the previous one and including it in the sum if it is different.  So if there wasn't an acceptance, then the point remains the same.  The ratio to the full length is the acceptance rate.  Maybe we should define a function here instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def acceptance_rate(trace_array):\n",
    "    \"\"\"Calculate how many times the entry in the trace array changed compared\n",
    "       to the total length.\n",
    "    \"\"\"\n",
    "    changed = np.sum(trace_array[1:] != trace_array[:-1])\n",
    "    total_length = trace_array.shape[0]\n",
    "    return changed / total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "acceptance_rate(trace['Mean of Data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Geweke Score\n",
    "\n",
    "From a different lecture:\n",
    "\n",
    "We can explicitly think of this test as a test for the Ergodicity (stationarity) of your chain.\n",
    "\n",
    "Take the first 10 and last 50% of your chain and do a z test comparing means (correcting for autocorrelation). Software packages, take this a step further: The geweke function in pymc3 by default chooses the first 10% of your chain, and the final 50%; divides the final 50% of the chain into 20 segments and performs a z-test for each segment. You want to fail to reject the null, since the hypothesis is:\n",
    "\n",
    "H0:θ10%=$θs50%H1:θ10%≠$θs50%\n",
    "for each segment s. If our means are the same (we fail to reject the null), then we have strong evidence of chain convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "score=pm.geweke(trace, first=0.1, last=0.5, intervals=20)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "score=pm.geweke(trace, first=0.1, last=0.5, intervals=20)\n",
    "plt.scatter(score[0]['Mean of Data'][:,0],score[0]['Mean of Data'][:,1], marker = 'o', s=100)\n",
    "plt.axhline(-1.98, c='r')\n",
    "plt.axhline(1.98, c='r')\n",
    "plt.ylim(-2.5,2.5)\n",
    "plt.xlim(0-10,.5*trace['Mean of Data'].shape[0]/2+10)\n",
    "plt.title('Geweke Plot Comparing first 10% and Slices of the Last 50% of Chain\\nDifference in Mean Z score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "score=pm.geweke(trace, first=0.0005, last=0.5, intervals=20)\n",
    "plt.scatter(score[0]['Mean of Data'][:,0],score[0]['Mean of Data'][:,1], \n",
    "            marker = 'o', s=100)\n",
    "plt.axhline(-1.98, c='r')\n",
    "plt.axhline(1.98, c='r')\n",
    "plt.ylim(-2.5,2.5)\n",
    "plt.xlim(0-10,.5*trace['Mean of Data'].shape[0]/2+10)\n",
    "my_title = 'Geweke Plot Comparing first 0.05% and Slices of the Last 50%' +\\\n",
    "           'of Chain\\nDifference in Mean Z score'\n",
    "plt.title(my_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "From a different lecture: \"Even without dropping any burn-in observations, we have convergence. We only start seeing issues when we restrict ourselves to the first 5 values in the chain. Suggests we should drop the first few dozen observations for burn-in.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gelman Rubin Diagnostic (quoted verbatim from the Hicks notebook)\n",
    "\n",
    "If our MH MCMC Chain reaches a stationary distribution, and we repeat the excercise multiple times, then we can examine if the posterior for each chain converges to the same place in the distribution of the parameter space.\n",
    "\n",
    "Steps:\n",
    "1. Run $M>1$ Chains of length $2 \\times N$.\n",
    "2. Discard the first $N$ draws of each chain, leaving $N$ iterations in the chain.\n",
    "3. Calculate the within and between chain variance.\n",
    "    * Within chain variance:\n",
    "    $$\n",
    "    W = \\frac{1}{M}\\sum_{j=1}^M s_j^2 \n",
    "    $$\n",
    "    where $s_j^2$ is the variance of each chain (after throwing out the first $N$ draws).\n",
    "    * Between chain variance:\n",
    "    $$\n",
    "    B = \\frac{N}{M-1} \\sum_{j=1}^M (\\bar{\\theta_j} - \\bar{\\bar{\\theta}})^2\n",
    "    $$\n",
    "    \n",
    "    where $\\bar{\\bar{\\theta}}$ is the mean of each of the M means.\n",
    "4. Calculate the estimated variance of $\\theta$ as the weighted sum of between and within chain variance.\n",
    "$$\n",
    "\\hat{var}(\\theta) = \\left ( 1 - \\frac{1}{N}\\right ) W + \\frac{1}{N}B\n",
    "$$\n",
    "5. Calculate the potential scale reduction factor.\n",
    "$$\n",
    "\\hat{R} = \\sqrt{\\frac{\\hat{var}(\\theta)}{W}}\n",
    "$$\n",
    "\n",
    "We want this number to be close to 1.  Why?  This would indicate that the between chain variance is small.  This makes sense, if between chain variance is small, that means both chains are mixing around the stationary distribution.  Gelmen and Rubin show that when $\\hat{R}$ is greater than 1.1 or 1.2, we need longer burn-in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "chain_length = 100000 \n",
    "\n",
    "with basic_model:\n",
    "    # obtain starting values via MAP\n",
    "    startvals = pm.find_MAP(model=basic_model)\n",
    "    print(startvals)\n",
    "    # instantiate sampler\n",
    "    step = pm.Metropolis() \n",
    "\n",
    "    # draw 5000 posterior samples\n",
    "    trace = pm.sample(chain_length, step=step, start=startvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace,figsize=(20,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.gelman_rubin(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"Based on Gelman-Rubin and Geweke, we can be confident we have a chain that has converged to the limiting distribution (although perhaps for Gelman-Rubin, we'd want to try different starting values rather than MAP).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Trying without `find_MAP`, i.e., not specifying `start` in `pm.sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "chain_length = 100000 \n",
    "\n",
    "with basic_model:\n",
    "    # obtain starting values via MAP\n",
    "    #startvals = pm.find_MAP(model=basic_model)\n",
    "    #print(startvals)\n",
    "    # instantiate sampler\n",
    "    step = pm.Metropolis() \n",
    "\n",
    "    # draw 5000 posterior samples\n",
    "    trace = pm.sample(chain_length, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace,figsize=(20,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.gelman_rubin(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"The diagnostics we have discussed are all univariate (they work perfectly when there is only 1 parameter to estimate). Other diagnostics have been derived for the multivariate case, but these are useful only when using Gibbs Samplers or other specialized versions of Metropolis-Hastings.\n",
    "\n",
    "So most people examine univariate diagnostics for each variable, examine autocorrelation plots, acceptance rates and try to argue chain convergence based on that- unless they are using Gibbs or other specialized samplers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In-class exercise\n",
    "\n",
    "Let's try to modify the code below to estimate sigma as well as the mean:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigma = 3.  # standard deviation\n",
    "mu = 10.    # mean\n",
    "num_samples = 100  # 10**6\n",
    "\n",
    "# sample from a normal distribution\n",
    "data = stats.norm(mu, sigma).rvs(num_samples)  \n",
    "\n",
    "\n",
    "# plot a histogram of the sampled data\n",
    "num_bins = 20\n",
    "plt.hist(data, bins=num_bins)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# parameters for the prior on mu\n",
    "mu_mean_prior = 8.\n",
    "mu_sd_prior = 1.5  # Note this is our prior on the std of mu\n",
    "\n",
    "with pm.Model() as basic_model:\n",
    "\n",
    "    # Priors for unknown model parameters\n",
    "    mu = pm.Normal('Mean of Data', mu_mean_prior, mu_sd_prior)\n",
    "    \n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    data_in = pm.Normal('Y_obs', mu=mu, sd=sigma, observed=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "chain_length = 10000 \n",
    "\n",
    "with basic_model:\n",
    "    # obtain starting values via MAP\n",
    "    startvals = pm.find_MAP(model=basic_model)\n",
    "    print(startvals)\n",
    "    # instantiate sampler\n",
    "    step = pm.Metropolis() \n",
    "\n",
    "    # draw 10000 posterior samples\n",
    "    trace = pm.sample(chain_length, step=step, start=startvals) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "score=pm.geweke(trace, first=0.1, last=0.5, intervals=20)\n",
    "plt.scatter(score[0]['Mean of Data'][:,0],score[0]['Mean of Data'][:,1], \n",
    "            marker = 'o', s=100)\n",
    "plt.axhline(-1.98, c='r')\n",
    "plt.axhline(1.98, c='r')\n",
    "plt.ylim(-2.5,2.5)\n",
    "plt.xlim(0-10,.5*trace['Mean of Data'].shape[0]/2+10)\n",
    "my_title = 'Geweke Plot Comparing first 10% and Slices of the Last 50%' +\\\n",
    "           ' of Chain\\nDifference in Mean Z score'\n",
    "plt.title(my_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ok, we're trying it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigma = 3.  # standard deviation\n",
    "mu = 10.    # mean\n",
    "num_samples = 1000  # 100 # 10**6\n",
    "\n",
    "# sample from a normal distribution\n",
    "data = stats.norm(mu, sigma).rvs(num_samples)  \n",
    "\n",
    "\n",
    "# plot a histogram of the sampled data\n",
    "num_bins = 20\n",
    "plt.hist(data, bins=num_bins)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# parameters for the prior on mu\n",
    "mu_mean_prior = 8.\n",
    "mu_sd_prior = 1.5  # Note this is our prior on the std of mu\n",
    "\n",
    "sigma_mean_prior = 1.\n",
    "sigma_sd_prior = 1.\n",
    "\n",
    "with pm.Model() as two_param_model:\n",
    "\n",
    "    # Priors for unknown model parameters\n",
    "    mu = pm.Normal('Mean of Data', mu_mean_prior, mu_sd_prior)\n",
    "    sigma = pm.Normal('SD of Data', sigma_mean_prior, sigma_sd_prior)\n",
    "    \n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    data_in = pm.Normal('Y_obs', mu=mu, sd=sigma, observed=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "chain_length = 10000 \n",
    "\n",
    "with two_param_model:\n",
    "    # obtain starting values via MAP\n",
    "    startvals = pm.find_MAP(model=two_param_model)\n",
    "    print(startvals)\n",
    "    # instantiate sampler\n",
    "    step = pm.Metropolis() \n",
    "\n",
    "    # draw 10000 posterior samples\n",
    "    trace_two_param = pm.sample(chain_length, step=step, start=startvals) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace_two_param, figsize=(20,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "score=pm.geweke(trace_two_param, first=0.1, last=0.5, intervals=20)\n",
    "plt.scatter(score[0]['Mean of Data'][:,0],score[0]['Mean of Data'][:,1], \n",
    "            marker = 'o', s=100)\n",
    "plt.axhline(-1.98, c='r')\n",
    "plt.axhline(1.98, c='r')\n",
    "plt.ylim(-2.5,2.5)\n",
    "plt.xlim(0-10,.5*trace['Mean of Data'].shape[0]/2+10)\n",
    "my_title = 'Geweke Plot Comparing first 10% and Slices of the Last 50%' +\\\n",
    "           ' of Chain\\nDifference in Mean Z score'\n",
    "plt.title(my_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=pm.geweke(trace_two_param, first=0.1, last=0.5, intervals=20)\n",
    "plt.scatter(score[0]['SD of Data'][:,0],score[0]['SD of Data'][:,1], marker = 'o', s=100)\n",
    "plt.axhline(-1.98, c='r')\n",
    "plt.axhline(1.98, c='r')\n",
    "plt.ylim(-2.5,2.5)\n",
    "#plt.xlim(0-10,.5*trace['SD of Data'].shape[0]/2+10)\n",
    "plt.title('Geweke Plot Comparing first 10% and Slices of the Last 50% of Chain\\nDifference in SD Z score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:talent-env]",
   "language": "python",
   "name": "conda-env-talent-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
