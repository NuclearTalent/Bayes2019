{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# A Bayesian Billiard game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Import of modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import special\n",
    "\n",
    "# Not really needed, but nicer plots\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Illustrative example: A Bayesian billiard game\n",
    "Adapted from the blog post [Frequentism and Bayesianism II: When Results Differ](http://jakevdp.github.io/blog/2014/06/06/frequentism-and-bayesianism-2-when-results-differ/) \n",
    "\n",
    "This example of nuisance parameters dates all the way back to the posthumous [1763 paper](http://www.stat.ucla.edu/history/essay.pdf) written by Thomas Bayes himself. The particular version of this problem used here is borrowed from [Eddy 2004](ftp://selab.janelia.org/pub/publications/Eddy-ATG3/Eddy-ATG3-reprint.pdf).\n",
    "\n",
    "The setting is a rather contrived game in which Alice and Bob bet on the outcome of a process they can't directly observe:\n",
    "\n",
    "Alice and Bob enter a room. Behind a curtain there is a billiard table, which they cannot see, but their friend Carol can. Carol rolls a ball down the table, and marks where it lands. Once this mark is in place, Carol begins rolling new balls down the table. If the ball lands to the left of the mark, Alice gets a point; if it lands to the right of the mark, Bob gets a point.  We can assume for the sake of example that Carol's rolls are unbiased: that is, the balls have an equal chance of ending up anywhere on the table.  The first person to reach **six points** wins the game.\n",
    "\n",
    "Here the location of the mark (determined by the first roll) can be considered a nuisance parameter: it is unknown, and not of immediate interest, but it clearly must be accounted for when predicting the outcome of subsequent rolls. If the first roll settles far to the right, then subsequent rolls will favor Alice. If it settles far to the left, Bob will be favored instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given this setup, here is the question we ask of ourselves:\n",
    "\n",
    "> In a particular game, after eight rolls, Alice has five points and Bob has three points. What is the probability that Bob will go on to win the game?\n",
    "\n",
    "Intuitively, you probably realize that because Alice received five of the eight points, the marker placement likely favors her. And given this, it's more likely that the next roll will go her way as well. And she has three opportunities to get a favorable roll before Bob can win; she seems to have clinched it.  But, **quantitatively**, what is the probability that Bob will squeak-out a win?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A Naive Frequentist Approach\n",
    "Someone following a classical frequentist approach might reason as follows:\n",
    "\n",
    "To determine the result, we need an intermediate estimate of where the marker sits. We'll quantify this marker placement as a probability $\\alpha$ that any given roll lands in Alice's favor.  Because five balls out of eight fell on Alice's side of the marker, we can quickly show that the maximum likelihood estimate of $\\alpha$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha} = 5/8\n",
    "$$\n",
    "\n",
    "(This result follows in a straightforward manner from the [binomial likelihood](http://en.wikipedia.org/wiki/Binomial_distribution)). Assuming this maximum likelihood estimate, we can compute the probability that Bob will win, which is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "p(B) = (1 - \\hat{\\alpha})^3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is, he needs to win three rolls in a row. Thus, we find that the following estimate of the probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive frequentist probability of Bob winning: 0.05\n",
      "or\n",
      "Odds against Bob winning: 18 to 1\n"
     ]
    }
   ],
   "source": [
    "alpha_hat = 5. / 8.\n",
    "freq_prob = (1 - alpha_hat) ** 3\n",
    "print(f\"Naive frequentist probability of Bob winning: {freq_prob:.2f}\")\n",
    "print(f\"or\\nOdds against Bob winning: {(1. - freq_prob) / freq_prob:.0f} to 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've estimated using frequentist ideas that Alice will win about 17 times for each time Bob wins. Let's try a Bayesian approach next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayesian approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also approach this problem from a Bayesian standpoint. This is slightly more involved, and requires us to first define some notation.\n",
    "\n",
    "We'll consider the following random variables:\n",
    "\n",
    "- $B$ = Bob Wins;\n",
    "- $D$ = observed data, i.e. $D = (n_A, n_B) = (5, 3)$;\n",
    "- $I$ = other information that we have, e.g. concerning the rules of the game;\n",
    "- $\\alpha$ = unknown probability that a ball lands on Alice's side during the current game.\n",
    "\n",
    "We want to compute $p(B~|~D,I)$; that is, the probability that Bob wins given our observation that Alice currently has five points to Bob's three.\n",
    "\n",
    "In the following, we will not explicitly include $I$ in the expressions for conditional probabilities. However, it should be assumed to be part of the known propositions, e.g. \n",
    "$$p(B~|~D)\\equiv p(B~|~D,I),$$\n",
    "$$p(\\alpha) \\equiv p(\\alpha~|~I),$$ etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The general Bayesian method of treating nuisance parameters is *marginalization*, or integrating the joint probability over the entire range of the nuisance parameter. In this case, that means that we will first calculate the joint distribution\n",
    "\n",
    "$$\n",
    "p(B,\\alpha~|~D)\n",
    "$$\n",
    "\n",
    "and then marginalize over $\\alpha$ using the following identity:\n",
    "\n",
    "$$\n",
    "p(B~|~D) \\equiv \\int_{-\\infty}^\\infty p(B,\\alpha~|~D) {\\mathrm d}\\alpha\n",
    "$$\n",
    "\n",
    "This identity follows from the definition of conditional probability, and the law of total probability: that is, it is a fundamental consequence of probability axioms and will always be true. Even a frequentist would recognize this; they would simply disagree with our interpretation of $p(\\alpha|I)$ (appearing below) as being a measure of uncertainty of our own knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Building our Bayesian Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute this result, we will manipulate the above expression for $p(B~|~D)$ until we can express it in terms of other quantities that we can compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by applying the following definition of [conditional probability](http://en.wikipedia.org/wiki/Conditional_probability#Definition) to expand the term $p(B,\\alpha~|~D)$:\n",
    "\n",
    "$$\n",
    "p(B~|~D) = \\int P(B~|~\\alpha, D) P(\\alpha~|~D) \\mathrm{d}\\alpha\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use [Bayes' rule](http://en.wikipedia.org/wiki/Bayes%27_theorem) to rewrite $p(\\alpha~|~D)$:\n",
    "\n",
    "$$\n",
    "p(B~|~D) = \\int p(B~|~\\alpha, D) \\frac{p(D~|~\\alpha)p(\\alpha)}{p(D)} \\mathrm{d}\\alpha\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, using the same probability identity we started with, we can expand $p(D)$ in the denominator to find:\n",
    "\n",
    "$$\n",
    "p(B~|~D) = \\frac{\\int p(B~|~\\alpha,D) p(D~|~\\alpha) p(\\alpha) \\mathrm{d}\\alpha}{\\int p(D~|~\\alpha)p(\\alpha) \\mathrm{d}\\alpha}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the desired probability is expressed in terms of three quantities that we can compute. Let's look at each of these in turn:\n",
    "\n",
    "- $p(B~|~\\alpha,D)$: This term is exactly the frequentist likelihood we used above. In words: given a marker placement $\\alpha$ and the fact that Alice has won 5 times and Bob 3 times, what is the probability that Bob will go on to six wins?  Bob needs three wins in a row, i.e. $p(B~|~\\alpha,D) = (1 - \\alpha) ^ 3$.\n",
    "- $p(D~|~\\alpha)$: this is another easy-to-compute term. In words: given a probability $\\alpha$, what is the likelihood of exactly 5 positive outcomes out of eight trials? The answer comes from the well-known [Binomial distribution](http://en.wikipedia.org/wiki/Binomial_distribution): in this case $p(D~|~\\alpha) \\propto \\alpha^5 (1-\\alpha)^3$\n",
    "- $p(\\alpha)$: this is our prior on the probability $\\alpha$. By the problem definition, we can assume that $\\alpha$ is evenly drawn between 0 and 1.  That is, $p(\\alpha)$ is a uniform probability distribution in the range from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Putting this all together, canceling some terms, and simplifying a bit, we find\n",
    "$$\n",
    "p(B~|~D) = \\frac{\\int_0^1 (1 - \\alpha)^6 \\alpha^5 \\mathrm{d}\\alpha}{\\int_0^1 (1 - \\alpha)^3 \\alpha^5 \\mathrm{d}\\alpha}\n",
    "$$\n",
    "where both integrals are evaluated from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These integrals are special cases of the [Beta Function](http://en.wikipedia.org/wiki/Beta_function):\n",
    "$$\n",
    "\\beta(n, m) = \\int_0^1 (1 - t)^{n - 1} t^{m - 1} dt\n",
    "$$\n",
    "The Beta function can be further expressed in terms of gamma functions (i.e. factorials), but for simplicity we'll compute them directly using Scipy's beta function implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(B|D) = 0.09\n",
      "or\n",
      "Bayesian odds against Bob winning: 10 to 1\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import beta\n",
    "bayes_prob = beta(6 + 1, 5 + 1) / beta(3 + 1, 5 + 1)\n",
    "\n",
    "print(f\"p(B|D) = {bayes_prob:.2f}\")\n",
    "print(f\"or\\nBayesian odds against Bob winning: {(1. - bayes_prob) / bayes_prob:.0f} to 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the Bayesian result gives us 10 to 1 odds, which is quite different than the 17 to 1 odds found using the frequentist approach. So which one is correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Brute-force (Monte Carlo) approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this type of well-defined and simple setup, it is actually relatively easy to use a Monte Carlo simulation to determine the correct answer. This is essentially a brute-force tabulation of possible outcomes: we generate a large number of random games, and simply count the fraction of relevant games that Bob goes on to win. The current problem is especially simple because so many of the random variables involved are uniformly distributed.  We can use the ``numpy`` package to do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Sanity check passed)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# play 100000 games with randomly-drawn p, between 0 and 1\n",
    "p = np.random.random(100000)\n",
    "\n",
    "# each game needs at most 11 rolls for one player to reach 6 wins\n",
    "rolls = np.random.random((11, len(p)))\n",
    "\n",
    "# count the cumulative wins for Alice and Bob at each roll\n",
    "Alice_count = np.cumsum(rolls < p, 0)\n",
    "Bob_count = np.cumsum(rolls >= p, 0)\n",
    "\n",
    "# sanity check: total number of wins should equal number of rolls\n",
    "total_wins = Alice_count + Bob_count\n",
    "assert np.all(total_wins.T == np.arange(1, 12))\n",
    "print(\"(Sanity check passed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of suitable games: 11068 (out of 100000 simulated ones)\n",
      "Number of these games Bob won: 979\n",
      "Monte Carlo Probability of Bob winning: 0.09\n",
      "MC Odds against Bob winning: 10 to 1\n"
     ]
    }
   ],
   "source": [
    "# determine number of games which meet our criterion of (A wins, B wins)=(5, 3)\n",
    "# this means Bob's win count at eight rolls must equal 3\n",
    "good_games = Bob_count[7] == 3\n",
    "print(f\"Number of suitable games: {good_games.sum()} (out of {len(p)} simulated ones)\")\n",
    "\n",
    "# truncate our results to consider only these games\n",
    "Alice_count = Alice_count[:, good_games]\n",
    "Bob_count = Bob_count[:, good_games]\n",
    "\n",
    "# determine which of these games Bob won.\n",
    "# to win, he must reach six wins after 11 rolls.\n",
    "bob_won = np.sum(Bob_count[10] == 6)\n",
    "print(\"Number of these games Bob won: {0}\".format(bob_won.sum()))\n",
    "\n",
    "# compute the probability\n",
    "mc_prob = bob_won.sum() * 1. / good_games.sum()\n",
    "print(f\"Monte Carlo Probability of Bob winning: {mc_prob:.2f}\")\n",
    "print(f\"MC Odds against Bob winning: {(1. - mc_prob) / mc_prob:.0f} to 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte Carlo approach gives 10-to-1 odds on Bob, which agrees with the Bayesian result. Apparently, our naive frequentist approach above was flawed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This example shows different approaches to dealing with the presence of a nuisance parameter $\\alpha$. The Monte Carlo simulation gives us a close brute-force estimate of the true probability (assuming the validity of our assumptions), which the Bayesian approach matches. The naive frequentist approach, by utilizing a single maximum likelihood estimate of the nuisance parameter $\\alpha$, arrives at the wrong result.\n",
    "\n",
    "We should emphasize that **this does not imply frequentism itself is incorrect**. The incorrect result above is more a matter of the approach being \"naive\" than it being \"frequentist\". There certainly exist frequentist methods for handling this sort of nuisance parameter – for example, it is theoretically possible to apply a transformation and conditioning of the data to isolate the dependence on $\\alpha$ – but it's hard to find any approach to this particular problem that does not somehow take advantage of Bayesian-like marginalization over $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Another potential point of contention is that the question itself is posed in a way that is perhaps unfair to the classical, frequentist approach. A frequentist might instead hope to give the answer in terms of null tests or confidence intervals: that is, they might devise a procedure to construct limits which would provably bound the correct answer in $100\\times(1 - p)$ percent of similar trials, for some value of $p$ – say, 0.05. This might be classically accurate, but it doesn't quite answer the question at hand. \n",
    "\n",
    "In contrast, Bayesianism provides a better approach for this sort of problem: by simple algebraic manipulation of a few well-known axioms of probability within a Bayesian framework, we can straightforwardly arrive at the correct answer without need for other special expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
