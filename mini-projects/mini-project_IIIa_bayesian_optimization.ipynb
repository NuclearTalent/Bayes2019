{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TALENT Course 11\n",
    "## Learning from Data: Bayesian Methods and Machine Learning\n",
    "### York, UK, June 10-28, 2019 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise and mini-project IIIa: Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to update the `talent-env` environment to use `GPyOpt` in this notebook. Please follow the instructions at: [https://nucleartalent.github.io/Bayes2019/installation/](https://nucleartalent.github.io/Bayes2019/installation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "import GPy\n",
    "import GPyOpt\n",
    "\n",
    "# Not really needed, but nicer plots\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A univariate example with GPyOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to minimize the function\n",
    "$$\n",
    "\\sin(3\\theta) + \\theta^2 - 0.7 \\theta\n",
    "$$\n",
    "on the interval $\\theta \\in [-1,2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. **Plot the true function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Find the minimum using `scipy.optimize.minimize`. Plot in the figure with the true function. Repeat with a few different seeds for the starting point. **Do you always get the same minimum?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Use Bayesian Optimization with GPyOpt (following the example in the lecture notebook).\n",
    "\n",
    "**Plot the statistical model and the acquisition function for the first ten iterations. Also plot the final summary of the BayesOpt convergence sequence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Change the acquisition function to 'LCB'. Make sure to plot the iterations. **How do the acquisition functions compare when it comes to exploration-exploitation?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. **Repeat with noise added to the true function when generating data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assuming that we have an input parameter vector `X`, and that we have defined `noise = 0.2`. Then we can create some noise with normal distribution using\n",
    "```python\n",
    "noise * np.random.randn(*X.shape)\n",
    "```\n",
    "* Redefine your \"true\" function so that it returns results with such noise and repeat the `GPyOpt` implementation.\n",
    "* It is important that your GP expects a noisy function. You must set `exact_feval = False`.\n",
    "* Plot several samples from the \"noisy\" true function (using e.g. `alpha=0.1` to make them semi-transparent). Also plot the true function without noise.\n",
    "* Perform the Bayesian optimization. Study the convergence, but also the statistical model. **How is it different compared to the statistical model in the example without noise?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. **Build the statistical model in BayesOpt with a different kernel.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try in particular with the `Matern32` kernel. Do you remember how it looks like?\n",
    "* Define a GPy kernel with your initial guess varaince and lengthscale\n",
    "```python\n",
    "GPkernel = GPy.kern.Matern32(input_dim=1, variance=1.0, lengthscale=1.0)\n",
    "```\n",
    "* Include this kernel as an input argument to `GPyOpt.methods.BayesianOptimization` \n",
    "```python\n",
    "optimizer = BayesianOptimization(f=fNoise, \n",
    "                                 model_type='GP',\n",
    "                                 kernel=GPkernel, \n",
    "                                 ...\n",
    "```\n",
    "**Questions**\n",
    "* Can you decide if any of these kernels work better for this problem then the other? \n",
    "* What is the observable difference between the posterior function in this experiment compared to the previous one with the default `RBF` kernel?\n",
    "* How would you decide which kernel to use for your problem?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build your own BayesOpt algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to repeat the above, but **assemble your own BayesOpt algorithm** using functions from `numpy`, `scipy`, and `GPy` (for building the statistical model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the pseudo-code for BayesOpt\n",
    "1. initial $\\mathbf{\\theta}^{(1)},\\mathbf{\\theta}^{(2)},\\ldots \\mathbf{\\theta}^{(k)}$, where $k \\geq 2$\n",
    "1. evaluate the objective function $f(\\mathbf{\\theta})$ to obtain $y^{(i)}=f(\\mathbf{\\theta}^{(i)})$ for $i=1,\\ldots,k$\n",
    "1. initialize a data vector $\\mathcal{D}_k = \\left\\{(\\mathbf{\\theta}^{(i)},y^{(i)})\\right\\}_{i=1}^k$\n",
    "1. select a statistical model for $f(\\mathbf{\\theta})$\n",
    "1. **For** {$n=k+1,k+2,\\ldots$}\n",
    "   1.    select $\\mathbf{\\theta}^{(n)}$ by optimizing the acquisition function: $\\mathbf{\\theta}^{(n)} = \\underset{\\mathbf{\\theta}}{\\text{arg max}}\\, \\mathcal{A}(\\mathbf{\\theta}|\\mathcal{D}_{n-1})$\n",
    "   1.    evaluate the objective function to obtain $y^{(n)}=f(\\mathbf{\\theta}^{(n)})$\n",
    "   1.    augment the data vector $\\mathcal{D}_n = \\left\\{\\mathcal{D}_{n-1} , (\\mathbf{\\theta}^{(n)},y^{(n)})\\right\\}$\n",
    "   1.    update the statistical model for $f(\\mathbf{\\theta})$\n",
    "1. **end for**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sub-tasks:**\n",
    "* You have to implement all steps in the above pseudo-code.\n",
    "* For the statistical model you can use `GPyOpt`, following the examples from last week's lecture and exercise. Remember that the model has to be updated at step 5D.\n",
    "* Implement the LCB acquisition function for use in step 5A. The maximum of $\\mathcal{A}(\\theta)$ can be found using `numpy.scipy.minimize` (note that you want the maximum...). It is a good idea to try several different starting points. See example code below, or implement your own algorithm if you prefer bug checking your own code.\n",
    "  As an alternative to LCB, if you have time, you can also try the implementation of Expected Improvement in the code snippet below. However, this code might have to be cleansed of bugs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code for a function that proposes the next sampling point by computing the location of the acquisition function maximum. Optimization is restarted `n_restarts` times to avoid local optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def propose_location(acquisition, X_sample, Y_sample, gpr, bounds, n_restarts=25):\n",
    "    ''' \n",
    "    Proposes the next sampling point by optimizing the acquisition function. \n",
    "    Args: \n",
    "    acquisition: Acquisition function. \n",
    "    X_sample: Sample locations (n x d). \n",
    "    Y_sample: Sample values (n x 1). \n",
    "    gpr: A GaussianProcessRegressor fitted to samples. \n",
    "    Returns: Location of the acquisition function maximum. \n",
    "    '''\n",
    "    dim = X_sample.shape[1]\n",
    "    min_val = 1\n",
    "    min_x = None\n",
    "    \n",
    "    def min_obj(X):\n",
    "        # Minimization objective is the negative acquisition function\n",
    "        return -acquisition(X.reshape(-1, dim), X_sample, Y_sample, gpr)\n",
    "    \n",
    "    # Find the best optimum by starting from n_restart different random points.\n",
    "    for x0 in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, dim)):\n",
    "        res = minimize(min_obj, x0=x0, bounds=bounds, method='L-BFGS-B')        \n",
    "        if res.fun < min_val:\n",
    "            min_val = res.fun[0]\n",
    "            min_x = res.x           \n",
    "            \n",
    "    return min_x.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code for the Expected Improvement acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def expected_improvement(X, X_sample, Y_sample, gpr, xi=0.01):\n",
    "    ''' \n",
    "    Computes the EI at points X based on existing samples, \n",
    "    X_sample and Y_sample, using a Gaussian process surrogate model. \n",
    "    Args: \n",
    "    X: Points at which EI shall be computed (m x d). \n",
    "    X_sample: Sample locations (n x d). \n",
    "    Y_sample: Sample values (n x 1). \n",
    "    m: A GP model from GPy fitted to samples. \n",
    "    xi: Exploitation-exploration trade-off parameter. \n",
    "    \n",
    "    Returns: Expected improvements at points X. \n",
    "    '''\n",
    "    (mu, sigma) = gpr.predict(X)\n",
    "    (mu_sample, sigma_sample) = gpr.predict(X_sample)\n",
    "\n",
    "    sigma = sigma.reshape(-1, X_sample.shape[1])\n",
    "    \n",
    "    # Needed for noise-based model,\n",
    "    # otherwise use np.max(Y_sample).\n",
    "    mu_sample_opt = np.max(mu_sample)\n",
    "\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu - mu_sample_opt - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "\n",
    "    return ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test on bivariate example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use your own BayesOpt implementation, or the GPy one, to find the minimum of the following objective function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langermann(x):\n",
    "    \"\"\"\n",
    "    Langermann test objective function.\n",
    "    Args: \n",
    "    x: Two-dimensional array \n",
    "    Returns: Function value \n",
    "    \"\"\"\n",
    "\n",
    "    a = [3,5,2,1,7]\n",
    "    b = [5,2,1,4,9]\n",
    "    c = [1,2,5,2,3]\n",
    "    \n",
    "    return -sum(c*np.exp(-(1/np.pi)*((x[0]-a)**2 \\\n",
    "                               + (x[1]-b)**2))*np.cos(np.pi*((x[0]-a)**2 \\\n",
    "                                + (x[1]-b)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to investigate different choices for the acquisition function and for the covariance function of your statistical model. In particular, be sure to compare the `RBF` and `Matern32` kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multivariate test examples (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you have time, try one of the challenging multivariate test functions that are presented in the Appendix of [Bayesian optimization in ab initio nuclear physics. arXiv:1902.00941](https://arxiv.org/abs/1902.00941)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "#\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
